Semi-Supervised Learning

Introduction

Machine learning algorithms fall under two categories based on how they “learn” the data they are given; they are either supervised or unsupervised. In both supervised an unsupervised learning, the algorithm infers a function to describe the data is has been given. The main difference between the two learning styles is that the data used to train the supervised learning algorithm has an output value (also known as a supervisory signal) in which the inferred function is trying to reproduce and the data used to train the unsupervised learning algorithm does not have an accompanying output value. Thus, the unsupervised learning algorithms are using the training data to find similarities in the data structure and responds to the presence/absence of these similarities when presented with new data.

Labeled data, data that has an accompanying output value, is not always readily available in the real world. If the data exists without the label, people can be paid to undertake the painstaking, time-consuming, and expensive task of analyzing each observation and declaring the appropriate label. However, this option is becoming less realistic as the collection and demand for data reaches ever-increasing levels. For this reason, companies such as Google, Microsoft, and Facebook are turning to an approach called semi-supervised learning.

Semi-supervised learning utilizes a supervised learning algorithm trained on ground-truth labeled data to predict the output values of the unlabeled data. These predicted output values are called pseudo-labels for the unlabeled data and can be considered as accurate as the model that was used to predict them. For example, if the model used has 85% accuracy when predicting the output value of the ground-truth labeled data, it can be assumed that 85% of the generated pseudo-labels are correct for the unlabeled data. 

The theory behind using semi-supervised learning is that it allows access to a greater amount of data to use while training a model. The more training data a model has access to equates to better model performance because the underlying algorithm has access to more information about the inherent structure and trends present in the data. Thus, the algorithm will be better able to infer a function to describe the data.

How to employ semi-supervised learning:

1.	Split the ground-truth labeled subset of the data into a training set and a holdout set.
2.	Train the model on the training set of ground-truth labeled subset until acceptable performance metrics levels are achieved.
3.	Present the unlabeled subset of the data to the trained model and use its predictions as the pseudo-labels for the unlabeled subset.
4.	Combine the ground-truth labeled and pseudo-labeled subsets and retrain the model with the combined data.
5.	Test the retrained model on the holdout set to determine final performance metrics.

Cautions and Pitfalls

While the promise of reducing costs, decreasing production time, and increasing performance metrics sounds enticing, semi-supervised learning may not be applicable for every situation as there are some drawbacks that need to be considered. Firstly, if the data is noisy to begin with, the accuracy of the pseudo-labels may be poor. These will in turn add more noise to the data and ultimately reduce the performance of the model. Furthermore, generating incorrect labels for data points and then retraining the model on these inaccurate labels will reinforce the aspects of the algorithm’s inferred function that generated this incorrect label, ultimately leading to a function that does not capture the underlying structure of the data. 

The best way to assess the performance of a semi-supervised learning model is to use a holdout set of ground-truth labeled data that the model did not see during training and compare its performance metrics. If the performance metrics of the model with the training data are significantly higher than the performance metrics for the model on the holdout set, then this suggest that the model’s function was not flexible enough to generalize to new data, and thus the pseudo-labels it generated are most likely inaccurate and introduced noise.

Summary

Semi-supervised learning utilizes the strengths of supervised learning to estimate labels for unlabeled data within a dataset. This reduces the overall cost and length of time of a project, while also usually increasing the performance of a model. However, the model used to generate the pseudo-labels for the unlabeled data needs to have a high level of accuracy in order to avoid introducing noise and reinforcing incorrect decision points within the model’s inferred function.
